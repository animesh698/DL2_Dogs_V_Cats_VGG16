import numpy as np
import keras
from keras import backend as K
from keras.models import Sequential
from keras.layers import Activation
from keras.layers.core import Dense, Flatten
from keras.optimizers import Adam
from keras.metrics import categorical_crossentropy
from keras.preprocessing.image import ImageDataGenerator
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import *
from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt
import itertools
import matplotlib.pyplot as plt

train_path='C:/Users/legend698/Desktop/train'
test_path='C:/Users/legend698/Desktop/ANIMESH/python/Cats-and-Dogs/test'
valid_path='C:/Users/legend698/Desktop/valid'

#Creating batches for train,test and valid 
#ImageDataGenerator is a keras object and generates batches of tensor image data and this is the format in which the iamges need to be in order to be read by keras
#flow_from_directory takes the path to a directory on disc where the images reside and generates batches of normalized data
#ImageDataGenerator generates batches of data from the images or path mentioned in the 1st arg
#classes param is optional,keras will infer the classes based on the structure of the directory of the train/test/valid images
train_batches=ImageDataGenerator().flow_from_directory(train_path,target_size=(224,224),classes=['dog','cat'],batch_size=10)
test_batches =ImageDataGenerator().flow_from_directory(test_path,target_size=(224,224),classes=['dog','cat'],batch_size=10)
valid_batches=ImageDataGenerator().flow_from_directory(valid_path,target_size=(224,224),classes=['dog','cat'],batch_size=10)

#Building a simple CNN
#1st layer is convolutional network,2d since we are dealing with 2d images
#Conv2D args-arg 1- no of output filters,arg 2-kernel size-tuple of 2 integers,that is the width and the height of the 2d conv window  
# model=Sequential([
# 	Conv2D(32,(3,3),activation='relu',input_shape=(224,224,3)),
# 	#Flatten layer will flatten the o/p from the previous layer into a 1D tensor that will be fed to the following dense layer
# 	Flatten(),
# 	Dense(2,activation='softmax'),
# 	])

#model.compile(Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])
#fit_generator fits the data generated by the DataGenerator batch by batch  	
#steps_per_epoch=training_set/batch_size
# model.fit_generator(train_batches,steps_per_epoch=500,validation_data=valid_batches,validation_steps=10,epochs=5,verbose=2)
#predict
#predict_generator produces predictions for our input samples from our Data gen ,our data gen in this case is going to be the test batches that we created with the ImageDataGenerator  
#Steps-same as steps per epoch,validation steps
# predictions=model.predict_generator(test_batches,steps=250,verbose=0)

#VGG model(trined on imagenet DS with over 100 classes)
vgg16_model=keras.applications.vgg16.VGG16()
#type of model is not sequential unlike what we have worked with,it is of type Model,hence we will transform it into a seq model
model = Sequential()
for layer in vgg16_model.layers:
	model.add(layer) #model has no layers we iterate over vgg and add it to sequ model

model.layers.pop() #removes last layer because the model is trained on 1000 categs and we need to determine only cats and dogs hence we pop the last layer
#pop always removes the last layer(seq model is a stack of layers)
for layer in model.layers:
	layer.trainable=False
	#we are excluding this layer from training so that its weights wont change ,useful for fine tuning of model

#Adding last layer with 2 classes
model.add(Dense(2,activation='softmax')) 
model.compile(Adam(0.0001),loss='categorical_crossentropy',metrics=['accuracy'])
model.fit_generator(train_batches,steps_per_epoch=2500,validation_data=valid_batches,validation_steps=40,epochs=5,verbose=2)
predictions=model.predict_generator(test_batches,verbose=0,steps=1250)
test_labels=test_labels[:,0]

#confusion matrix
# cm_plot_labels=['cat','dog']
cm=confusion_matrix(test_labels,np.round(predictions[:,0]))
# plot_confusion_matrix(cm,classes,title='Confudion Matrix')
def plot_confusion_matrix(cm,classes,normalize=False,title='Confusion Matrix',cmap=plt.cm.Blues):
	plt.imshow(cm,interpolation='nearest',cmap=cmap)
	plt.title(title)
	plt.colorbar()
	tick_marks=np.arange(len(classes))
	plt.xticks(tick_marks,classes,rotation=45)
	plt.yticks(tick_marks,classes)

	if normalize:
		cm=cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]
		print("Normalized confusion matrix")
	else:
		print('Confusion Matrix,without normalization')

	print(cm)

	thresh = cm.max() / 2.
	for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):
		plt.text(j,i,cm[i,j],horizontalalignment="center",color="white" if cm[i,j]>thresh else "black")
	plt.tight_layout()
	plt.ylabel('True Label')
	plt.xlabel('Predicted Label')
cm_plot_labels=['cat','dog']
plot_confusion_matrix(cm,cm_plot_labels,title='Confusion Matrix')
plt.show()
model.save('VGG.h5')



#To view how keras assigns label to classes in our data,"classes.indices" under image processing/ImageDataGenerator/flow_from_director/
#it gives the index of the label in the 1 hot encoded array 

#Reproducable results
#when you train the model multiple times at diff times,the acc,loss might vary.This is because the weights assigned initially are randomly generated,
#Then will gradually change via gradient descent
#Set random seeds for the randomly generated variables by python,numpy or tensorflow
#1)Setting seeds for random generates numpy -generated random numbers 2) py random nos 3)restricting tf to use a single thread 

#biases in keras
#if not set to true by default true
#bias_initializers-'zeros' bias can be other values as well.Biases will be updates in a similar way as weights

#learnable parameters
#no if inputs+no of outputs +biases<-dense layer
#CNN has a diff process bcz filters or kernels are involved


#Max Pooling layer
#Max Pooling is added to CNNs following individual conv layers
#When added to a model max pooling reduces the dimensionality of images by reducing the pixels from the o/p of the previous conv layer

